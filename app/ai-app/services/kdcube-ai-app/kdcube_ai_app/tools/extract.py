# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Elena Viter

# tools/data_processing.py
import os
import subprocess
import sys
import tempfile
import time
from urllib.parse import urlparse

from typing import Dict, Union, Any, List, Tuple

from kdcube_ai_app.tools.processing import DataSourceExtractionResult

import logging
logger = logging.getLogger("ExtractTool")

def get_filename_from_path(file_path: str) -> str:
    """Extract filename with extension from path or URL."""
    parsed = urlparse(file_path)

    if parsed.scheme in ('file', 'http', 'https', 's3'):
        # For URLs, extract from the path component
        path_part = parsed.path
    else:
        # For regular filesystem paths
        path_part = file_path

    # Get just the filename
    return os.path.basename(path_part)

# 1. Content fetching (handles downloading external resources)

class PDFExtractor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def extract(self,
                content: Union[str, bytes],
                file_path: str) -> List[DataSourceExtractionResult]:
        """Extract content from PDF using marker library, including images and other assets."""
        try:
            from marker.converters.pdf import PdfConverter
            from marker.models import create_model_dict
            from marker.output import text_from_rendered
        except ImportError:
            PdfConverter = None
            create_model_dict = None
            text_from_rendered = None

        # Initialize marker converter
        marker_path = os.environ.get("MARKER_BINARY_PATH", None)
        if not marker_path:
            logger.warning("MARKER_BINARY_PATH not set, using default marker binary path")
            marker_path = os.path.join(sys.prefix, "bin", "marker_single")

        env = os.environ.copy()
        env.pop("DEBUG", None)  # keep Marker from choking on DEBUG=""

        tmp_dir = tempfile.TemporaryDirectory()
        logger.info(f"Using temp dir {tmp_dir.name}")
        output_dir = os.path.join(tmp_dir.name, "output")

        filename = get_filename_from_path(file_path)
        input_file_path = os.path.join(tmp_dir.name, "input", filename)
        os.makedirs(os.path.dirname(input_file_path), exist_ok=True)

        # Write input file
        with open(input_file_path, 'wb') as f:
            if isinstance(content, bytes):
                f.write(content)
            else:
                f.write(content.encode('utf-8'))

        start_time = time.time()
        # Run marker extraction
        result = subprocess.run(
            [
                marker_path,
                input_file_path,
                "--output_dir", output_dir,
                "--output_format", "markdown",
            ],
            capture_output=True,
            text=True,
            env=env
        )
        end_time = time.time()
        execution_time = end_time - start_time

        self.logger.info(f"Extracted {filename} in {execution_time:.2f} seconds")
        self.logger.debug(result.stdout)
        self.logger.debug(result.stderr)


        if result.returncode != 0:
            e = Exception(f"Marker conversion failed: {result.stderr}")
            logger.error(e)
            return []

        try:
            # Get all files produced by marker
            all_files = self._collect_marker_output_files(output_dir)

            if not all_files["markdown"]:
                logger.error("No markdown files generated by marker")
                return []

            # Process each markdown file and its associated assets
            extraction_results = []

            for md_index, md_file in enumerate(all_files["markdown"]):
                # Read markdown content
                with open(os.path.join(output_dir, md_file), 'r', encoding='utf-8') as f:
                    markdown_content = f.read()

                # Process and update markdown content to reference stored assets
                processed_content, asset_mappings = self._process_markdown_assets(
                    markdown_content, all_files, md_index, output_dir
                )

                # Create result with comprehensive metadata
                metadata = {
                    "source_file": file_path,
                    "filename": filename,
                    "extraction_index": md_index,
                    "markdown_file": md_file,
                    "assets": asset_mappings,
                    "type": "markdown",
                    "marker_output": {
                        "total_images": len(all_files["images"]),
                        "total_tables": len(all_files["tables"]),
                        "total_metadata_files": len(all_files["metadata"]),
                        "other_files": all_files["other"]
                    }
                }

                extraction_results.append(
                    DataSourceExtractionResult(
                        content=processed_content,
                        metadata=metadata
                    )
                )

            return extraction_results

        finally:
            # Cleanup temporary directory
            try:
                tmp_dir.cleanup()
            except Exception as e:
                logger.error(f"Error cleaning up marker output directory {tmp_dir.name}: {str(e)}")

    def _collect_marker_output_files(self, output_dir: str) -> Dict[str, List[str]]:
        """Collect and categorize all files produced by marker."""

        all_files = {
            "markdown": [],
            "images": [],
            "tables": [],
            "metadata": [],
            "other": []
        }

        # Common image extensions
        image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp'}
        # Common table/data extensions
        table_extensions = {'.csv', '.tsv', '.xlsx'}
        # Metadata extensions
        metadata_extensions = {'.json', '.xml', '.yaml', '.yml'}

        # Walk through all files in output directory
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                file_path = os.path.relpath(os.path.join(root, file), output_dir)
                file_ext = os.path.splitext(file)[1].lower()

                if file_ext == '.md':
                    all_files["markdown"].append(file_path)
                elif file_ext in image_extensions:
                    all_files["images"].append(file_path)
                elif file_ext in table_extensions:
                    all_files["tables"].append(file_path)
                elif file_ext in metadata_extensions:
                    all_files["metadata"].append(file_path)
                else:
                    all_files["other"].append(file_path)

        # Sort files for consistent ordering
        for category in all_files:
            all_files[category].sort()

        logger.info(f"Marker output collected: {len(all_files['markdown'])} markdown, "
                    f"{len(all_files['images'])} images, {len(all_files['tables'])} tables, "
                    f"{len(all_files['metadata'])} metadata, {len(all_files['other'])} other files")

        return all_files

    def _process_markdown_assets(self, markdown_content: str, all_files: Dict[str, List[str]],
                                 md_index: int, output_dir: str) -> Tuple[str, Dict[str, Any]]:
        """
        Process markdown content to handle asset references and prepare asset mappings.

        Returns:
            Tuple of (processed_markdown_content, asset_mappings)
        """
        import re

        asset_mappings = {
            "images": [],
            "tables": [],
            "metadata": [],
            "other": []
        }

        processed_content = markdown_content

        # Find all image references in markdown (![alt](path) format)
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        image_matches = re.findall(image_pattern, markdown_content)

        for alt_text, img_path in image_matches:
            # Try to find the actual image file
            original_img_path = img_path

            # Look for this image in our collected files
            matching_images = [img for img in all_files["images"]
                               if os.path.basename(img_path) == os.path.basename(img)]

            if matching_images:
                actual_img_path = matching_images[0]

                # Generate new filename for storage
                img_ext = os.path.splitext(actual_img_path)[1]
                new_img_filename = f"extraction_{md_index}_image_{len(asset_mappings['images'])}{img_ext}"

                # Read image content
                full_img_path = os.path.join(output_dir, actual_img_path)
                if os.path.exists(full_img_path):
                    with open(full_img_path, 'rb') as f:
                        img_content = f.read()

                    # Add to asset mappings
                    asset_mappings["images"].append({
                        "original_path": original_img_path,
                        "actual_path": actual_img_path,
                        "storage_filename": new_img_filename,
                        "alt_text": alt_text,
                        "content": img_content,
                        "size_bytes": len(img_content)
                    })

                    # Update markdown content to reference new path
                    processed_content = processed_content.replace(
                        f"![]({original_img_path})",
                        f"![{alt_text}]({new_img_filename})"
                    )

        # Process other asset types that might be referenced
        for table_file in all_files["tables"]:
            table_ext = os.path.splitext(table_file)[1]
            new_table_filename = f"extraction_{md_index}_table_{len(asset_mappings['tables'])}{table_ext}"

            full_table_path = os.path.join(output_dir, table_file)
            if os.path.exists(full_table_path):
                with open(full_table_path, 'rb') as f:
                    table_content = f.read()

                asset_mappings["tables"].append({
                    "original_path": table_file,
                    "storage_filename": new_table_filename,
                    "content": table_content,
                    "size_bytes": len(table_content)
                })

        # Process metadata files
        for metadata_file in all_files["metadata"]:
            metadata_ext = os.path.splitext(metadata_file)[1]
            new_metadata_filename = f"extraction_{md_index}_metadata_{len(asset_mappings['metadata'])}{metadata_ext}"

            full_metadata_path = os.path.join(output_dir, metadata_file)
            if os.path.exists(full_metadata_path):
                with open(full_metadata_path, 'rb') as f:
                    metadata_content = f.read()

                asset_mappings["metadata"].append({
                    "original_path": metadata_file,
                    "storage_filename": new_metadata_filename,
                    "content": metadata_content,
                    "size_bytes": len(metadata_content)
                })

        # Process other files
        for other_file in all_files["other"]:
            other_ext = os.path.splitext(other_file)[1]
            new_other_filename = f"extraction_{md_index}_asset_{len(asset_mappings['other'])}{other_ext}"

            full_other_path = os.path.join(output_dir, other_file)
            if os.path.exists(full_other_path):
                with open(full_other_path, 'rb') as f:
                    other_content = f.read()

                asset_mappings["other"].append({
                    "original_path": other_file,
                    "storage_filename": new_other_filename,
                    "content": other_content,
                    "size_bytes": len(other_content)
                })

        return processed_content, asset_mappings